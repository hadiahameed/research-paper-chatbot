{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x7FpYeuNWOlK"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements1.txt --default-timeout=900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hplWR21B12Dh",
    "outputId": "f3fc342b-f395-4a3b-f8e2-060c827c23f3"
   },
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "\n",
    "#----------------------------------Importing Packages----------------------------------\n",
    "# A vector store takes care of storing embedded data and performing vector search for you.\n",
    "#Options are Chroma, FAISS (Facebook AI Similarity Search) or Lance\n",
    "#Chroma DB is for storing and retrieving vector embeddings and runs on local machine.\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "#Embeddings create a vector representation of a piece of text.\n",
    "#sentence_transformers package models are originating from Sentence-BERT\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "#ChatOpenAI is the primary class used for chatting with OpenAI models.\n",
    "#This represents LangChain's interface for interacting with OpenAI's API.\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "#Does question answering over documents you pass in, and cites it sources.\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "#Use document loaders to load data from a source as Document's. A Document is a piece of text and associated metadata.\n",
    "#For example, there are document loaders for loading a simple .txt file, for loading the text contents of any web page,\n",
    "#or even for loading a transcript of a YouTube video.\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "#Split a long document into smaller chunks that can fit into your model's context window.\n",
    "#Recursively splits text. Splitting text recursively serves the purpose of trying to keep related pieces of text next to\n",
    "#each other. This is the recommended way to start splitting text.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "import openai\n",
    "import streamlit as sm\n",
    "import os\n",
    "import shutil\n",
    "import hashlib\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "#---------------------------------- Code----------------------------------\n",
    "sm.title(\"I am LabBot - your personal research assistant!\")\n",
    "\n",
    "openai.api_key = '<YOUR KEY HERE>' #get your own key from OpenAI\n",
    "os.environ['OPENAI_API_KEY'] = openai.api_key\n",
    "tmp_directory = 'temp'\n",
    "\n",
    "new_uploads = \"Upload new research papers.\"\n",
    "uploaded_papers = \"Uploaded papers.\"\n",
    "\n",
    "chatbot_started = False\n",
    "\n",
    "#Function for uploading new .txt files\n",
    "def doc_upload(directory: str):\n",
    "    documents = []\n",
    "    loader = DirectoryLoader(path=directory, show_progress=True)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "#Splitting documents into small chunks\n",
    "def doc_split(documents, chunk_size=1500, chunk_overlap=30):\n",
    "    split_text = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    docs = split_text.split_documents(documents)\n",
    "\n",
    "    return docs\n",
    "\n",
    "@sm.cache_resource #only loads if there is a change in update\n",
    "#Wrapper around OpenAI\n",
    "def startup_event(last_update: str):\n",
    "\n",
    "    print(f\"{last_update}\")\n",
    "    directory = 'temp/'\n",
    "    documents = doc_upload(directory)\n",
    "    docs = doc_split(documents)\n",
    "\n",
    "    #----------------------------------Chroma db----------------------------------\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\") #small LLM\n",
    "    persist_directory = \"chroma_db\"\n",
    "\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "\n",
    "    vectordb.persist()\n",
    "    db = Chroma.from_documents(docs, embeddings) #on disk\n",
    "\n",
    "    #----------------------------------GPT Model----------------------------------\n",
    "    model_name = \"gpt-3.5-turbo\"\n",
    "    llm = ChatOpenAI(model_name=model_name)\n",
    "    chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\", verbose=True)\n",
    "\n",
    "    return db, chain\n",
    "\n",
    "#---------------------------------- Querying the GPT Model with RAG---------------\n",
    "def get_answer(query: str, db, chain):\n",
    "    matching_docs_score = db.similarity_search_with_score(query)\n",
    "\n",
    "    matching_docs = [doc for doc, score in matching_docs_score]\n",
    "\n",
    "    ##LLM will create a prompt for ChatGPT using these docs to build context and add our query to it.\n",
    "    answer = chain.run(input_documents=matching_docs, question=query)\n",
    "\n",
    "\n",
    "    sources = [{\n",
    "        \"content\": doc.page_content,\n",
    "        \"metadata\": doc.metadata,\n",
    "        \"score\": score\n",
    "    } for doc, score in matching_docs_score]\n",
    "\n",
    "    return {\"answer\": answer, \"sources\": sources}\n",
    "\n",
    "\n",
    "#UI of the ChatBot\n",
    "def start_chatbot():\n",
    "    db, chain = startup_event(last_db_updated)\n",
    "    if \"openai_model\" not in sm.session_state:\n",
    "        sm.session_state[\"openai_model\"] = \"gpt-3.5-turbo\"\n",
    "\n",
    "    if \"messages\" not in sm.session_state:\n",
    "        sm.session_state.messages = []\n",
    "\n",
    "    for message in sm.session_state.messages:\n",
    "        with sm.chat_message(message[\"role\"]):\n",
    "            sm.markdown(message[\"content\"])\n",
    "\n",
    "    if prompt := sm.chat_input(\"What is up?\"):\n",
    "        sm.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        with sm.chat_message(\"user\"):\n",
    "            sm.markdown(prompt)\n",
    "\n",
    "        with sm.chat_message(\"assistant\"):\n",
    "            message_placeholder = sm.empty()\n",
    "            full_response = get_answer(sm.session_state.messages[-1][\"content\"], db, chain)\n",
    "            answer = full_response['answer']\n",
    "            message_placeholder.markdown(answer)\n",
    "            sm.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "content_type = sm.sidebar.radio(\"Which knowledge base do you want to use?\",\n",
    "                                [uploaded_papers, new_uploads])\n",
    "if content_type == new_uploads:\n",
    "    uploaded_files = sm.sidebar.file_uploader(\"Choose a txt file\", type=\"txt\", accept_multiple_files=True)\n",
    "    uploaded_file_names = [file.name for file in uploaded_files]\n",
    "\n",
    "    if uploaded_files is not None and len(uploaded_files):\n",
    "        if os.path.exists(tmp_directory):\n",
    "            shutil.rmtree(tmp_directory)\n",
    "\n",
    "        os.makedirs(tmp_directory)\n",
    "\n",
    "        if len(uploaded_files):\n",
    "            last_db_updated = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "\n",
    "        for file in uploaded_files:\n",
    "            with open(f\"{tmp_directory}/{file.name}\", 'wb') as tmp:\n",
    "                tmp.write(file.getvalue())\n",
    "                tmp.seek(0)\n",
    "curr_dir = [path.split(os.path.sep)[-1] for path in glob(tmp_directory + '/*')]\n",
    "\n",
    "if content_type == uploaded_papers:\n",
    "    sm.sidebar.write(\"Current Knowledge Base\")\n",
    "    if len(curr_dir):\n",
    "        sm.sidebar.write(curr_dir)\n",
    "    else:\n",
    "        sm.sidebar.write('**No KB Uploaded**')\n",
    "\n",
    "last_db_updated = hashlib.md5(','.join(curr_dir).encode()).hexdigest()\n",
    "\n",
    "if curr_dir and len(curr_dir):\n",
    "    start_chatbot()\n",
    "else:\n",
    "    sm.header('No KB Loaded, use left menu to start.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H0ajvkFhReS1",
    "outputId": "eeb1d9ea-c544-41f3-e525-6780a6b7b5c0"
   },
   "outputs": [],
   "source": [
    "!streamlit run app.py"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
